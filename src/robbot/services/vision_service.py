"""
VisionService para an√°lise de imagens usando BLIP-2 (open source, local, SEM CUSTO).

Alternativa local ao Gemini Vision.
Usa Salesforce BLIP-2 para image captioning e visual question answering.
"""

import logging
import os
import tempfile
from typing import Optional

import httpx
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

logger = logging.getLogger(__name__)


class VisionService:
    """
    Servi√ßo para an√°lise de imagens usando BLIP-2 (local, sem custo).
    
    Modelo: Salesforce/blip-image-captioning-base
    - Open source (BSD-3 License)
    - Roda localmente (CPU ou GPU)
    - ~990MB download inicial
    - Zero custo de API
    - Qualidade boa para captioning
    
    Funcionalidades:
    - Gerar descri√ß√£o autom√°tica de imagens
    - Visual Question Answering (VQA)
    - An√°lise contextual para sa√∫de/emagrecimento
    """
    
    def __init__(self):
        """Inicializar modelo BLIP-2 (lazy loading)."""
        self.processor = None
        self.model = None
        self.model_name = "Salesforce/blip-image-captioning-base"
        
    def _load_model(self):
        """Carregar modelo BLIP-2 sob demanda."""
        if self.model is None:
            logger.info(f"üîÑ Carregando modelo BLIP-2: {self.model_name} (~990MB)...")
            try:
                self.processor = BlipProcessor.from_pretrained(self.model_name)
                self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)
                logger.info("‚úì Modelo BLIP-2 carregado com sucesso!")
            except Exception as e:
                logger.error(f"‚úó Erro ao carregar BLIP-2: {e}")
                raise
    
    async def analyze_image(
        self, 
        image_url: str,
        context: str = "medical",
        questions: Optional[list[str]] = None
    ) -> dict[str, str]:
        """
        Analisar imagem e gerar descri√ß√£o detalhada.
        
        Args:
            image_url: URL da imagem
            context: Contexto da an√°lise (medical, fitness, food, etc)
            questions: Perguntas espec√≠ficas para VQA
            
        Returns:
            Dict com:
            - caption: Descri√ß√£o principal da imagem
            - detailed_description: An√°lise detalhada
            - tags: Tags relevantes
            - answers: Respostas para perguntas (se fornecidas)
        """
        self._load_model()
        
        try:
            # 1. Baixar imagem
            logger.info(f"üì• Baixando imagem de: {image_url}")
            image = await self._download_image(image_url)
            
            # 2. Gerar caption b√°sico
            caption = await self._generate_caption(image)
            logger.info(f"‚úì Caption gerado: {caption}")
            
            # 3. Gerar descri√ß√£o detalhada com perguntas contextuais
            detailed_description = await self._generate_detailed_description(
                image, 
                caption, 
                context
            )
            
            # 4. Extrair tags da descri√ß√£o
            tags = self._extract_tags(caption, detailed_description, context)
            
            # 5. Responder perguntas customizadas (VQA)
            answers = {}
            if questions:
                for question in questions:
                    answer = await self._answer_question(image, question)
                    answers[question] = answer
            
            return {
                "caption": caption,
                "detailed_description": detailed_description,
                "tags": tags,
                "answers": answers
            }
            
        except Exception as e:
            logger.error(f"‚úó Erro ao analisar imagem: {e}")
            raise
    
    async def _download_image(self, url: str) -> Image.Image:
        """Baixar imagem de URL."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url)
            response.raise_for_status()
            
            # Salvar temporariamente
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
                tmp.write(response.content)
                tmp_path = tmp.name
            
            # Abrir com PIL
            image = Image.open(tmp_path).convert('RGB')
            
            # Limpar arquivo tempor√°rio
            os.unlink(tmp_path)
            
            return image
    
    async def _generate_caption(self, image: Image.Image) -> str:
        """Gerar caption b√°sico da imagem."""
        # Processar imagem
        inputs = self.processor(image, return_tensors="pt")
        
        # Gerar caption
        output = self.model.generate(**inputs, max_new_tokens=50)
        caption = self.processor.decode(output[0], skip_special_tokens=True)
        
        return caption
    
    async def _generate_detailed_description(
        self, 
        image: Image.Image, 
        caption: str,
        context: str
    ) -> str:
        """
        Gerar descri√ß√£o detalhada com perguntas contextuais.
        
        Para contexto m√©dico/emagrecimento, faz perguntas como:
        - "What type of food is shown?"
        - "Is this a healthy meal?"
        - "What activities are visible?"
        """
        if context == "medical":
            questions = [
                "What is the main subject of this image?",
                "What details are visible?",
                "What colors and objects are present?"
            ]
        else:
            questions = [
                "What is shown in this image?",
                "What are the key details?"
            ]
        
        descriptions = [caption]
        
        for question in questions:
            answer = await self._answer_question(image, question)
            descriptions.append(answer)
        
        return " ".join(descriptions)
    
    async def _answer_question(self, image: Image.Image, question: str) -> str:
        """Responder pergunta sobre a imagem (VQA)."""
        # BLIP suporta conditional generation com prompt
        prompt = f"Question: {question} Answer:"
        
        inputs = self.processor(image, text=prompt, return_tensors="pt")
        output = self.model.generate(**inputs, max_new_tokens=30)
        answer = self.processor.decode(output[0], skip_special_tokens=True)
        
        # Remover o prompt da resposta
        answer = answer.replace(prompt, "").strip()
        
        return answer
    
    def _extract_tags(self, caption: str, description: str, context: str) -> str:
        """Extrair tags relevantes da descri√ß√£o."""
        text = f"{caption} {description}".lower()
        
        # Tags base
        tags = ["imagem", "an√°lise visual"]
        
        # Palavras-chave contextuais para sa√∫de/emagrecimento
        health_keywords = {
            'food': 'alimenta√ß√£o',
            'meal': 'refei√ß√£o',
            'plate': 'prato',
            'salad': 'salada',
            'fruit': 'fruta',
            'vegetable': 'vegetal',
            'exercise': 'exerc√≠cio',
            'workout': 'treino',
            'gym': 'academia',
            'running': 'corrida',
            'walking': 'caminhada',
            'person': 'pessoa',
            'woman': 'mulher',
            'man': 'homem',
            'healthy': 'saud√°vel',
            'diet': 'dieta',
            'nutrition': 'nutri√ß√£o',
        }
        
        for keyword, tag in health_keywords.items():
            if keyword in text:
                tags.append(tag)
        
        # Adicionar contexto
        if context == "medical":
            tags.append("contexto m√©dico")
        
        return ", ".join(tags[:10])  # M√°ximo 10 tags
    
    def analyze_image_sync(
        self, 
        image_url: str,
        context: str = "medical"
    ) -> dict[str, str]:
        """
        Vers√£o s√≠ncrona de analyze_image.
        
        √ötil para jobs em background (RQ).
        """
        import asyncio
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            self.analyze_image(image_url, context)
        )


# Singleton para reutilizar modelo carregado
_vision_service_instance: Optional[VisionService] = None


def get_vision_service() -> VisionService:
    """Obter inst√¢ncia singleton do VisionService."""
    global _vision_service_instance
    
    if _vision_service_instance is None:
        _vision_service_instance = VisionService()
    
    return _vision_service_instance
